{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate HRU at different complexity levels ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt \n",
    "sys.path.append('../')\n",
    "import functions.geospatial_analysis as ga\n",
    "import functions.utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common paths\n",
    "control_file    = '../control.tpl.txt'\n",
    "basin_data_path = ut.read_from_control(control_file, 'basin_data_path')\n",
    "source_path     = ut.read_from_control(control_file, 'source_path')\n",
    "basin_name      = ut.read_from_control(control_file, 'basin_name')\n",
    "main_path       = ut.read_from_control(control_file, 'main_path')\n",
    "results_path    = os.path.join(basin_data_path, 'results/')\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basin data files and fields\n",
    "basin_gru_raster       = ut.set_filename(control_file, 'basin_gru_raster')\n",
    "basin_gru_corr_txt     = ut.set_filename(control_file, 'basin_gru_corr_txt')\n",
    "basin_dem_raster       = ut.set_filename(control_file, 'basin_dem_raster')  \n",
    "basin_slope_raster     = ut.set_filename(control_file, 'basin_slope_raster')  \n",
    "basin_aspect_raster    = ut.set_filename(control_file, 'basin_aspect_raster')\n",
    "basin_soil_raster      = ut.set_filename(control_file, 'basin_soil_raster')\n",
    "basin_radiation_raster = ut.set_filename(control_file, 'basin_radiation_raster')\n",
    "refraster              = ut.read_from_control(control_file, 'refraster')\n",
    "basin_landcover_class_raster = ut.set_filename(control_file, 'basin_landcover_class_raster')\n",
    "\n",
    "# derived filenames\n",
    "basin_gru_shp     = ut.read_from_control(control_file, 'basin_gru_shp')  \n",
    "basin_gru_prj_shp = basin_gru_shp.split('.shp')[0]+'_prj.shp' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru fieldnames\n",
    "gruNo_field           = ut.read_from_control(control_file, 'gruNo_field')\n",
    "gruNo_field_dtype     = ut.read_from_control(control_file, 'gruNo_field_dtype')\n",
    "gruName_field         = ut.read_from_control(control_file, 'gruName_field')\n",
    "\n",
    "# hru field names\n",
    "hruNo_field           = ut.read_from_control(control_file, 'hruNo_field')\n",
    "hruNo_field_dtype     = ut.read_from_control(control_file, 'hruNo_field_dtype')     # used to save hruNo raster. \n",
    "hruName_field         = ut.read_from_control(control_file, 'hruName_field')         # field name of hru name, e.g., 10080012010101, 100800120102. \n",
    "    \n",
    "elev_class_field      = ut.read_from_control(control_file, 'elev_class_field')      # field name of the elevation class column in HRU. \n",
    "land_class_field      = ut.read_from_control(control_file, 'land_class_field')      # field name of the land class column in HRU. \n",
    "radiation_class_field = ut.read_from_control(control_file, 'radiation_class_field') # field name of the radiation class column in HRU. \n",
    "hru_area_field        = ut.read_from_control(control_file, 'hru_area_field')        # field name of the HRU area.\n",
    "\n",
    "hru_thld_type         = ut.read_from_control(control_file,'hru_thld_type')          # use a fraction or area value to eliminate small HRUs.\n",
    "hru_thld              = float(ut.read_from_control(control_file, 'hru_thld'))       # if hru_thld_type = 'fraction', hru_thld = partial of the gru area.\n",
    "                                                                                    ## if hru_thld_type = 'value', hru_thld = elimination area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basename for dem and radiation classification in a hru level.\n",
    "dem_class_basename = 'dem_class' # basename for DEM class files (eg, 0:low elevation. 1: high elevation).\n",
    "dem_value_basename = 'dem_value' # basename for DEM value files (eg, average DEM per class).\n",
    "\n",
    "rad_class_basename = 'rad_class' # basename for radiation class files (eg, 0:low. 1:high).\n",
    "rad_value_basename = 'rad_value' # basename for radiation value files (eg, average radiation per class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define HRU complexity levels ####\n",
    "level 0: GRU = HRU. <br>\n",
    "level 1: use only elevation bands in HRU generation.<br>\n",
    "level 2a: use elevation bands and landcover classes in HRU generation.<br>\n",
    "level 2b: use elevation bands and radiation classes in HRU generation.<br>\n",
    "level 3a: use elevation bands, radiation bands, landcover classes in HRU generation.<br>\n",
    "level 3b: nested. radiation bands are generated based on level 2a HRU.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level_list = ['0','1','2a','2b','3a','3b']\n",
    "level_list = ['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Discretize HRU ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Complexity level 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'100800140106'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/ncar/usr/jupyterhub/envs/pangeo-2019.09.12/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '100800140106'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-274e43c75f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                      \u001b[0mhruNo_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhruNo_field_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhruName_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhru_area_field\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                      \u001b[0mfieldname_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefraster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                                      hru_vector_elmn, hru_raster_elmn)\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;31m#     ga.plot_vector(hru_vector_elmn, hruName_field) # quick plot for check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/proj/SHARP/wreg/complexity/watershed_tools/functions/geospatial_analysis.py\u001b[0m in \u001b[0;36meliminate_small_hrus_neighbor\u001b[0;34m(hru_vector, hru_thld_type, hru_thld, gruNo_field, gruName_field, hruNo_field, hruNo_field_dtype, hruName_field, hruArea_field, fieldname_list, refraster, hru_vector_disv, hru_raster_disv)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mflt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0min_gpd_disv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgruNo_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# filter 1: gru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mgru_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_gpd_disv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflt1\u001b[0m\u001b[0;34m]\u001b[0m                           \u001b[0;31m# gru dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0mdom_hru_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhruArea_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# the most dominant HRU index in gru_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0mdom_hru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdom_hru_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhruName_field\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;31m# the most dominant HRU hruName in gru_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ncar/usr/jupyterhub/envs/pangeo-2019.09.12/lib/python3.7/site-packages/geopandas/geodataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mgeo_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geometry_column_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgeo_col\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ncar/usr/jupyterhub/envs/pangeo-2019.09.12/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ncar/usr/jupyterhub/envs/pangeo-2019.09.12/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '100800140106'"
     ]
    }
   ],
   "source": [
    "for level in level_list:\n",
    "\n",
    "    print('--- Complexity level %s ---' %(level))\n",
    "    \n",
    "    #  --- PART 1. define output files of discretization--- \n",
    "    hru_str         = 'hru_lev' + str(level)\n",
    "    hru_elmn_str    = hru_str + '_elmn'     \n",
    "    \n",
    "    hru_raster      = os.path.join(results_path, hru_str+'.tif')       # original HRU\n",
    "    hru_vector      = os.path.join(results_path, hru_str+'.shp')\n",
    "    hru_raster_elmn = os.path.join(results_path, hru_elmn_str+'.tif')  # simplified HRU\n",
    "    hru_vector_elmn = os.path.join(results_path, hru_elmn_str+'.shp')    \n",
    "\n",
    "    dem_classif_trigger = 300 # Elvation difference value per GRU used to trigger elevation classification.\n",
    "    dem_bins            = 'median' # Elevation classification method. 'median' means using the median value per GRU as the classification threhold.\n",
    "    dem_class_raster    = os.path.join(results_path, dem_class_basename+'_lev'+str(level)+'.tif')\n",
    "    dem_value_raster    = os.path.join(results_path, dem_value_basename+'_lev'+str(level)+'.tif')\n",
    "    \n",
    "    rad_classif_trigger = 50 #None # Radiation difference value per GRU used to trigger elevation classification.\n",
    "    rad_bins            = 'median' # Radiation classification method. 'median' means using the median value per GRU as the classification threhold.\n",
    "    rad_class_raster    = os.path.join(results_path, rad_class_basename+'_lev'+str(level)+'.tif')\n",
    "    rad_value_raster    = os.path.join(results_path, rad_value_basename+'_lev'+str(level)+'.tif')\n",
    "    \n",
    "    #  --- PART 2. define inputs of discretization ---\n",
    "    #   raster_list: a list of raster inputs that are used to define HRU.\n",
    "    #   fieldname_list: a list of field names corresponding to raster_list.\n",
    "    \n",
    "    # level 0: GRU = HRU (benchmark). \n",
    "    if level == '0': \n",
    "        # (1) define input files for hru discretization\n",
    "        raster_list    = [basin_gru_raster]\n",
    "        fieldname_list = [gruNo_field]  \n",
    "\n",
    "    # level 1: use only elevation bands in HRU generation.\n",
    "    if level == '1': \n",
    "        # (1) classify elevation raster per gru\n",
    "        ga.classify_raster(basin_dem_raster, basin_gru_raster, dem_classif_trigger, dem_bins,\n",
    "                           dem_class_raster, dem_value_raster)        \n",
    "        # (2) define input files for hru discretization\n",
    "        raster_list    = [basin_gru_raster, dem_class_raster]\n",
    "        fieldname_list = [gruNo_field, elev_class_field]\n",
    "    \n",
    "    # level 2a: use elevation bands and landcover classes in HRU generation.\n",
    "    elif level == '2a': \n",
    "        # (1) classify elevation raster per gru\n",
    "        ga.classify_raster(basin_dem_raster, basin_gru_raster, dem_classif_trigger, dem_bins,\n",
    "                           dem_class_raster, dem_value_raster)        \n",
    "        # (2) define input files for hru discretization\n",
    "        raster_list    = [basin_gru_raster, dem_class_raster, basin_landcover_class_raster]\n",
    "        fieldname_list = [gruNo_field, elev_class_field, land_class_field]\n",
    "\n",
    "    # level 2b: use elevation bands and radiation classes in HRU generation.\n",
    "    elif level == '2b': \n",
    "        # (1) classify elevation raster per gru\n",
    "        ga.classify_raster(basin_dem_raster, basin_gru_raster, dem_classif_trigger, dem_bins,\n",
    "                           dem_class_raster, dem_value_raster)        \n",
    "        # (2) classify radiation raster per gru\n",
    "        ga.classify_raster(basin_radiation_raster, basin_gru_raster, rad_classif_trigger, rad_bins, \n",
    "                           rad_class_raster, rad_value_raster)        \n",
    "        # (3) define input files for hru discretization\n",
    "        raster_list    = [basin_gru_raster, dem_class_raster, rad_class_raster]\n",
    "        fieldname_list = [gruNo_field, elev_class_field, radiation_class_field]\n",
    "    \n",
    "    \n",
    "    # level 3a: use elevation bands, radiation bands, landcover classes in HRU generation.\n",
    "    elif level == '3a':\n",
    "        # (1) classify elevation raster per gru\n",
    "        ga.classify_raster(basin_dem_raster, basin_gru_raster, dem_classif_trigger, dem_bins,\n",
    "                           dem_class_raster, dem_value_raster)        \n",
    "        # (2) classify radiation raster per gru\n",
    "        ga.classify_raster(basin_radiation_raster, basin_gru_raster, rad_classif_trigger, rad_bins, \n",
    "                           rad_class_raster, rad_value_raster)        \n",
    "        # (3) define input files for hru discretization\n",
    "        raster_list    = [basin_gru_raster, dem_class_raster, rad_class_raster, basin_landcover_class_raster]\n",
    "        fieldname_list = [gruNo_field, elev_class_field, radiation_class_field, land_class_field]\n",
    "\n",
    "    # level 3b: nested. radiation bands are generated based on level 2a HRU.\n",
    "    elif level == '3b':\n",
    "        # (1) classify elevation raster per gru\n",
    "        ga.classify_raster(basin_dem_raster, basin_gru_raster, dem_classif_trigger, dem_bins, \n",
    "                                      dem_class_raster, dem_value_raster)        \n",
    "        # (2) classify radiation raster per level 2a hru\n",
    "        hru_lev2a_raster = os.path.join(domain_path, 'hru'+'_lev2a_elmn.tif')\n",
    "        ga.classify_raster(basin_radiation_raster, hru_lev2a_raster, rad_classif_trigger, rad_bins, \n",
    "                                      rad_class_raster, rad_value_raster)        \n",
    "        # (3) define input files for hru discretization\n",
    "        raster_list    = [basin_gru_raster, dem_class_raster, rad_class_raster, basin_landcover_class_raster]\n",
    "        fieldname_list = [gruNo_field, elev_class_field, radiation_class_field, land_class_field]\n",
    "\n",
    "    # --- PART 3. genearte HRU based on gru and elevation class ---\n",
    "    ga.define_hru(raster_list, fieldname_list, basin_gru_raster, basin_gru_corr_txt, gruNo_field, gruName_field,\n",
    "                  hru_raster, hru_vector, hruNo_field, hruNo_field_dtype, hruName_field)\n",
    "\n",
    "    # --- PART 4. calculate HRU area ---\n",
    "    in_gpd                 = gpd.read_file(hru_vector)\n",
    "    in_gpd[hru_area_field] = in_gpd.area\n",
    "    in_gpd.to_file(hru_vector)\n",
    "\n",
    "    # --- PART 5. eliminate small area HRUs ---\n",
    "    # mehod 1: change HRU attribute to the most dominant HRU within the GRU. Use function ga.eliminate_small_hrus_dominant\n",
    "    # method 2: change HRU attribute to its' largest neighbor's HRU\n",
    "    ga.eliminate_small_hrus_neighbor(hru_vector, hru_thld_type, hru_thld, gruNo_field, gruName_field, \n",
    "                                     hruNo_field, hruNo_field_dtype, hruName_field, hru_area_field, \n",
    "                                     fieldname_list, refraster, \n",
    "                                     hru_vector_elmn, hru_raster_elmn)\n",
    "#     ga.plot_vector(hru_vector_elmn, hruName_field) # quick plot for check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/work/andywood/complexity/basins/bighorn/06282000/results/hru_lev1.shp fraction 0.05 gruNo gruId hruNo int64 hruId 100800140106 ['gruNo', '100800140106'] basin_dem.tif /glade/work/andywood/complexity/basins/bighorn/06282000/results/hru_lev1_elmn.shp /glade/work/andywood/complexity/basins/bighorn/06282000/results/hru_lev1_elmn.tif\n"
     ]
    }
   ],
   "source": [
    "print(hru_vector, hru_thld_type, hru_thld, gruNo_field, gruName_field, \n",
    "                                     hruNo_field, hruNo_field_dtype, hruName_field, hru_area_field, \n",
    "                                     fieldname_list, refraster, \n",
    "                                     hru_vector_elmn, hru_raster_elmn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate HRU zonal statistics ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in level_list:\n",
    "\n",
    "    print('--- Complexity level %s ---' %(level))\n",
    "    \n",
    "    #  --- PART 1. define hru complexity level dependent files --- \n",
    "    hru_str = 'hru'+'_lev' + str(level)\n",
    "    hru_elmn_str = hru_str+'_elmn'     \n",
    "    \n",
    "    hru_vector = os.path.join(domain_path, hru_str+'.shp')\n",
    "    hru_vector_elmn = os.path.join(domain_path, hru_elmn_str+'.shp')    \n",
    "    \n",
    "    # --- PART 2. calculate zonal statistics ---\n",
    "    for invector in [hru_vector,hru_vector_elmn]:\n",
    "        \n",
    "        # (1) define invector dependent files \n",
    "        invector_field = hruNo_field\n",
    "        invector_field_dtype = hruNo_field_dtype\n",
    "        \n",
    "        attrb_elev = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_elevation.tif')        \n",
    "        attrb_slp = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_slope.tif')        \n",
    "        attrb_asp = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_aspect.tif')        \n",
    "        attrb_lc = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_landcover.tif')        \n",
    "        attrb_soil = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_soil.tif')        \n",
    "        \n",
    "        # (2) elevation zonal statistics \n",
    "        ga.zonal_statistic(domain_dem_raster, invector, invector_field, invector_field_dtype, \n",
    "                           refraster, 'mean', attrb_elev, output_column_prefix='elev')\n",
    "\n",
    "        # (3) slope zonal statistics \n",
    "        ga.zonal_statistic(domain_slope_raster, invector, invector_field, invector_field_dtype, refraster, \n",
    "                           'mean', attrb_slp, output_column_prefix='slope')\n",
    "\n",
    "        # (4) aspect zonal statistics \n",
    "        ga.zonal_statistic(domain_aspect_raster, invector, invector_field, invector_field_dtype, refraster, \n",
    "                           'mean_aspect', attrb_asp, output_column_prefix='aspect')\n",
    "\n",
    "        # (5) landcover zonal statistics \n",
    "        ga.zonal_statistic(domain_landcover_class_raster, invector, invector_field, invector_field_dtype, refraster, \n",
    "                           'mode', attrb_lc, output_column_prefix='vegType')        \n",
    "        \n",
    "        # (6) soil zonal statistics \n",
    "        ga.zonal_statistic(domain_soil_raster, invector, invector_field, invector_field_dtype, refraster, \n",
    "                           'mode', attrb_soil, output_column_prefix='soilType')        \n",
    "        \n",
    "        # -------- post-process attributes for SUMMA ---------\n",
    "        # (7) landcover and soil types \n",
    "        # convert landcover int to [1,17] range \n",
    "        # change soilType from float to int (because source soilType is float)\n",
    "        in_gpd = gpd.read_file(invector)\n",
    "        in_gpd['vegType'] = in_gpd['vegType']+1\n",
    "        in_gpd['soilType'] = in_gpd['soilType'].astype('int')\n",
    "        in_gpd.to_file(invector)\n",
    "        \n",
    "        # (8) convert landcover int to string for easy understanding\n",
    "        lcClass_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 255]\n",
    "        lcValue_list = ['Evergreen needleleaf forests', 'Evergreen broadleaf forests', 'Deciduous needleleaf forests',\n",
    "                        'Deciduous broadleaf forests', 'Mixed forests', 'Closed shrublands', 'Open shrublands', \n",
    "                        'Woody savannas', 'Savannas', 'Grasslands', 'Permanent wetlands', 'Croplands', \n",
    "                        'Urban and built-up lands', 'Cropland/natural vegetation mosaics', 'Snow and ice', \n",
    "                        'Barren', 'Water bodies', 'None']\n",
    "        in_gpd = gpd.read_file(invector)\n",
    "        in_gpd['landcover'] = \"\"\n",
    "        for irow, row in in_gpd.iterrows():\n",
    "            lcClass = in_gpd.loc[irow,'vegType'] \n",
    "            lcValue=lcValue_list[lcClass_list.index(lcClass)]\n",
    "            in_gpd.at[irow,'landcover'] = lcValue\n",
    "        in_gpd['landcover'] = in_gpd['landcover'].astype('str')\n",
    "        in_gpd.to_file(invector)\n",
    "\n",
    "        # (9) convert ROSETTA soil to STAS and add string for easy understanding\n",
    "        soilClass_list = [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "        soilValue_list = ['OTHER(land-ice)', 'CLAY', 'CLAY LOAM', 'LOAM', 'LOAMY SAND', 'SAND', 'SANDY CLAY', \n",
    "                          'SANDY CLAY LOAM', 'SANDY LOAM', 'SILT','SILTY CLAY', 'SILTY CLAY LOAM', 'SILT LOAM']\n",
    "        soilClass_list_STAS = [16, 12, 9, 6, 2, 1, 10, 7, 3, 5, 11, 8, 4]\n",
    "        \n",
    "        in_gpd = gpd.read_file(invector)\n",
    "        in_gpd['soilROSETTA'] = in_gpd['soilType']\n",
    "        in_gpd['soilSTAS'] = \"\"\n",
    "        in_gpd['soil'] = \"\"\n",
    "        for irow, row in in_gpd.iterrows():\n",
    "            \n",
    "            soilClass = in_gpd.loc[irow,'soilType'] \n",
    "            if soilClass==0:\n",
    "                lcClass = in_gpd.loc[irow,'vegType'] \n",
    "                print('hruNo = %d, soilType_ROSETTA = 0, and vegType = %s.'%(in_gpd.loc[irow,'hruNo'],lcClass))\n",
    "            \n",
    "            soilValue=soilValue_list[soilClass_list.index(soilClass)]\n",
    "            soilClass_STAS=soilClass_list_STAS[soilClass_list.index(soilClass)]\n",
    "            \n",
    "            in_gpd.at[irow,'soil'] = soilValue\n",
    "            in_gpd.at[irow,'soilSTAS'] = soilClass_STAS\n",
    "            \n",
    "        in_gpd['soil'] = in_gpd['soil'].astype('str')\n",
    "        in_gpd['soilSTAS'] = in_gpd['soilSTAS'].astype('int')\n",
    "        in_gpd['soilType'] = in_gpd['soilSTAS']\n",
    "        in_gpd = in_gpd.drop(columns=['soilSTAS'])\n",
    "        in_gpd.to_file(invector)\n",
    "\n",
    "        # (10) convert slope to tan_slope \n",
    "        in_gpd = gpd.read_file(invector)\n",
    "        in_gpd['tan_slope'] = np.tan(np.radians(in_gpd['slope']))\n",
    "        in_gpd.to_file(invector)\n",
    "\n",
    "        # (11) calculate contourLength (meter)\n",
    "        # assuming the hru area is a circle and taking the radius as contourLength.\n",
    "        in_gpd = gpd.read_file(invector)\n",
    "        in_gpd['contourLength'] = np.power(in_gpd['areaSqm']/np.pi,0.5)\n",
    "        in_gpd.to_file(invector)\n",
    "\n",
    "        # (12) calculate centroid lat/lon (degree)\n",
    "        def getXY(pt):\n",
    "            return (pt.x, pt.y)\n",
    "        in_gpd = gpd.read_file(invector)\n",
    "        in_gpd_wgs84 = in_gpd.copy()\n",
    "        in_gpd_wgs84 = in_gpd_wgs84.to_crs(epsg=4326) #\"EPSG:4326\"\n",
    "        centroidseries = in_gpd_wgs84['geometry'].centroid\n",
    "        in_gpd['longitude'],in_gpd['latitude']=[list(t) for t in zip(*map(getXY, centroidseries))]\n",
    "        in_gpd.to_file(invector)\n",
    "\n",
    "    # --- PART 3. save HRU with attributes into gpkg ---\n",
    "    invector_gpkg = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'.gpkg')\n",
    "    in_gpd = gpd.read_file(invector)\n",
    "    in_gpd.to_file(invector_gpkg, driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate GRU zonal statistics (optional) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invector = domain_gru_prj_shp\n",
    "invector_field = gruNo_field\n",
    "invector_field_dtype = gruNo_field_dtype\n",
    "\n",
    "attrb_elev = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_elevation.tif')        \n",
    "attrb_slp = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_slope.tif')        \n",
    "attrb_asp = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_aspect.tif')        \n",
    "attrb_lc = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_landcover.tif')        \n",
    "attrb_soil = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb_soil.tif')        \n",
    "\n",
    "outvector = os.path.join(domain_path, os.path.basename(invector).split('.shp')[0]+'_attrb.gpkg')     \n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd.to_file(outvector, driver=\"GPKG\")\n",
    "invector = outvector # avoid process gru_shp_prj. Work on gpkg.\n",
    "\n",
    "# (1) calculate zonal area ---\n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd['areaSqm'] = in_gpd.area\n",
    "in_gpd.to_file(invector)\n",
    "\n",
    "# (2) elevation zonal statistics \n",
    "ga.zonal_statistic(domain_dem_raster, invector, invector_field, invector_field_dtype, refraster, 'mean', attrb_elev,\n",
    "                   output_column_prefix='elev')\n",
    "\n",
    "# (3) slope zonal statistics \n",
    "ga.zonal_statistic(domain_slope_raster, invector, invector_field, invector_field_dtype, refraster, 'mean', attrb_slp,\n",
    "                   output_column_prefix='slope')\n",
    "\n",
    "# (4) aspect zonal statistics \n",
    "ga.zonal_statistic(domain_aspect_raster, invector, invector_field, invector_field_dtype, refraster, 'mean_aspect', attrb_asp,\n",
    "                   output_column_prefix='aspect')\n",
    "\n",
    "# (5) landcover zonal statistics \n",
    "ga.zonal_statistic(domain_landcover_class_raster, invector, invector_field, invector_field_dtype, refraster, 'mode', attrb_lc,\n",
    "                   output_column_prefix='vegType')        \n",
    "\n",
    "# (6) soil zonal statistics \n",
    "ga.zonal_statistic(domain_soil_raster, invector, invector_field, invector_field_dtype, refraster, 'mode', attrb_soil,\n",
    "                   output_column_prefix='soilType')        \n",
    "\n",
    "# -------- post-process attributes for SUMMA ---------\n",
    "# (7) landcover and soil types \n",
    "# convert landcover int to [1,17] range \n",
    "# change soilType from float to int (because source soilType is float)\n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd['vegType'] = in_gpd['vegType']+1\n",
    "in_gpd['soilType'] = in_gpd['soilType'].astype('int')\n",
    "in_gpd.to_file(invector)\n",
    "\n",
    "# (8) convert landcover int to string for easy understanding\n",
    "lcClass_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 255]\n",
    "lcValue_list = ['Evergreen needleleaf forests', 'Evergreen broadleaf forests', 'Deciduous needleleaf forests',\n",
    "                'Deciduous broadleaf forests', 'Mixed forests', 'Closed shrublands', 'Open shrublands', \n",
    "                'Woody savannas', 'Savannas', 'Grasslands', 'Permanent wetlands', 'Croplands', \n",
    "                'Urban and built-up lands', 'Cropland/natural vegetation mosaics', 'Snow and ice', \n",
    "                'Barren', 'Water bodies', 'None']\n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd['landcover'] = \"\"\n",
    "for irow, row in in_gpd.iterrows():\n",
    "    lcClass = in_gpd.loc[irow,'vegType'] \n",
    "    lcValue=lcValue_list[lcClass_list.index(lcClass)]\n",
    "    in_gpd.at[irow,'landcover'] = lcValue\n",
    "in_gpd['landcover'] = in_gpd['landcover'].astype('str')\n",
    "in_gpd.to_file(invector)\n",
    "\n",
    "# (9) convert ROSETTA soil to STAS and add string for easy understanding\n",
    "soilClass_list = [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "soilValue_list = ['OTHER(land-ice)', 'CLAY', 'CLAY LOAM', 'LOAM', 'LOAMY SAND', 'SAND', 'SANDY CLAY', \n",
    "                  'SANDY CLAY LOAM', 'SANDY LOAM', 'SILT','SILTY CLAY', 'SILTY CLAY LOAM', 'SILT LOAM']\n",
    "soilClass_list_STAS = [16, 12, 9, 6, 2, 1, 10, 7, 3, 5, 11, 8, 4]\n",
    "\n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd['soilROSETTA'] = in_gpd['soilType']\n",
    "in_gpd['soilSTAS'] = \"\"\n",
    "in_gpd['soil'] = \"\"\n",
    "for irow, row in in_gpd.iterrows():\n",
    "\n",
    "    soilClass = in_gpd.loc[irow,'soilType'] \n",
    "    if soilClass==0:\n",
    "        lcClass = in_gpd.loc[irow,'vegType'] \n",
    "        print('hruNo = %d, soilType_ROSETTA = 0, and vegType = %s.'%(in_gpd.loc[irow,'hruNo'],lcClass))\n",
    "\n",
    "    soilValue=soilValue_list[soilClass_list.index(soilClass)]\n",
    "    soilClass_STAS=soilClass_list_STAS[soilClass_list.index(soilClass)]\n",
    "\n",
    "    in_gpd.at[irow,'soil'] = soilValue\n",
    "    in_gpd.at[irow,'soilSTAS'] = soilClass_STAS\n",
    "\n",
    "in_gpd['soil'] = in_gpd['soil'].astype('str')\n",
    "in_gpd['soilSTAS'] = in_gpd['soilSTAS'].astype('int')\n",
    "in_gpd['soilType'] = in_gpd['soilSTAS']\n",
    "in_gpd = in_gpd.drop(columns=['soilSTAS'])\n",
    "in_gpd.to_file(invector)\n",
    "\n",
    "# (10) convert slope to tan_slope \n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd['tan_slope'] = np.tan(np.radians(in_gpd['slope']))\n",
    "in_gpd.to_file(invector)\n",
    "\n",
    "# (11) calculate contourLength (meter)\n",
    "# assuming the hru area is a circle and taking the radius as contourLength.\n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd['contourLength'] = np.power(in_gpd['areaSqm']/np.pi,0.5)\n",
    "in_gpd.to_file(invector)\n",
    "\n",
    "# (12) calculate centroid lat/lon (degree)\n",
    "def getXY(pt):\n",
    "    return (pt.x, pt.y)\n",
    "in_gpd = gpd.read_file(invector)\n",
    "in_gpd_wgs84 = in_gpd.copy()\n",
    "in_gpd_wgs84 = in_gpd_wgs84.to_crs(epsg=4326) #\"EPSG:4326\"\n",
    "centroidseries = in_gpd_wgs84['geometry'].centroid\n",
    "in_gpd['longitude'],in_gpd['latitude']=[list(t) for t in zip(*map(getXY, centroidseries))]\n",
    "in_gpd.to_file(invector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pangeo (2019.09.12 - py3.7)",
   "language": "python",
   "name": "pangeo-2019.09.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
